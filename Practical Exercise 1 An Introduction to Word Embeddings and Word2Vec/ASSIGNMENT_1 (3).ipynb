{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ighIhhiLaBtR"
      },
      "source": [
        "## Practical Exercise 1: An Introduction to Word Embeddings and Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKl0OAYcvl2"
      },
      "source": [
        "Before we start, we would like to highlight that each of the three notebooks will contribute equally to your final grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCmr1p64aBtW"
      },
      "source": [
        "This practical Exercise is presented as an IPython Notebook, with the code written for recent versions of **Python 3**.\n",
        "\n",
        "Before working with this notebook, you need to execute some of the pre-coded cells (to load libraries/functions/modules etc.). To execute one notebook cell, press `shift-enter`. The return value of the last command will be displayed, if it is not `None`.\n",
        "\n",
        "Below is a list with potentially useful library documentation, references, and resources:\n",
        "\n",
        "* IPython notebooks: <https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction>\n",
        "* Numpy numerical array library: <https://docs.scipy.org/doc/>\n",
        "* Gensim's word2vec: <https://radimrehurek.com/gensim/models/word2vec.html>\n",
        "* Bokeh interactive plots: <http://bokeh.pydata.org/en/latest/> (we provide plotting code here, but click the thumbnails for more examples to copy-paste)\n",
        "* scikit-learn ML library (aka `sklearn`): <http://scikit-learn.org/stable/documentation.html>\n",
        "* nltk NLP toolkit: <http://www.nltk.org/>\n",
        "* tutorial for processing xml in python using `lxml`: <http://lxml.de/tutorial.html> (we did this for you below, but in case you need it in the future)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introduction & imports"
      ],
      "metadata": {
        "id": "3-9ymQ4XRUtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOOU1S7cvl8"
      },
      "source": [
        "In this notebook you will familiarize yourselves with constructing and using word-embeddings. As you recall from the lecture, word-embeddings are a type of word representation that allow words with similar meaning to have similar representations. This can be achieved by representing words as real-valued vectors in a predefined vector space. However, before delving into the embedding process, you will need to learn how to use some basic NLP tools like tokenization and regular expressions!\n",
        "Good Luck and happy coding!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569W6MImaBtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2ae465-1bc1-4745-89fb-971d9834e733"
      },
      "source": [
        "# Importing necessary libraries/modules; requires to be executed once for every session\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from random import shuffle\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofHeuGejaZHs"
      },
      "source": [
        "# Here we install nltk. You only have to execute this cell once!\n",
        "try:\n",
        "    import nltk\n",
        "except:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install nltk\n",
        "    import nltk\n",
        "    nltk.download()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haav4K7YaBti"
      },
      "source": [
        "### Part 0: Download the TED dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXCrpUdjcvmM"
      },
      "source": [
        "For this practical exercise, we need a large amount of text data. We will use the TED database, which are the transcripts of Ted Talks. The next cells will download everything you need, this might take a while as the dataset is 75MB large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stf464BBaBtj"
      },
      "source": [
        "import zipfile\n",
        "import lxml.etree"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91T_rkPbaBtn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d1ccb315-8af9-42b2-816c-462ff13b56f6"
      },
      "source": [
        "# Upload the dataset if it's not already there: this may take a minute..\n",
        "if not os.path.isfile('ted_en-20160408.zip'):\n",
        "  from google.colab import files\n",
        "  # select the file \"ted_en-20160408.zip\" from your local drive here\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-82c7ba1c-3f19-4097-bed0-12a5b200faa6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-82c7ba1c-3f19-4097-bed0-12a5b200faa6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ted_en-20160408 (2).zip to ted_en-20160408 (2).zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWf1JkvaBtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed5c4d1-4937-4fb5-afbc-0d8f1633d282"
      },
      "source": [
        "# For now, we're only interested in the subtitle text, so let's extract that from the XML:\n",
        "with zipfile.ZipFile('ted_en-20160408 (2).zip', 'r') as z:\n",
        "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
        "input_text = '\\n'.join(doc.xpath('//content/text()'))\n",
        "\n",
        "print(type(input_text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give you a little bit of an idea about the structure of the dataset, you can open the `xml` file inside the zip folder and have a look at it. The dataset consists of metadata or properties of TedTalks, such as speaker information, language, transcripts, tags, date, and much more.\n",
        "\n",
        "As you can see, the `xml` file has lots of different tags to mark the beginning and end of the particular datapoint. The code cell bellow shows the tags that are inside the `xml` file. As you can already imagine, the tags are used to mark different properties of the data. For this assignment, we are interested in the content/text only, which is the reason for extracting the text from the content tag and from it, create the text data."
      ],
      "metadata": {
        "id": "LNL6FNYQA9QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all the tags in the XML\n",
        "tags = [element.tag for element in doc.iter()]\n",
        "\n",
        "# Get unique tags using a set\n",
        "unique_tags = set(tags)\n",
        "\n",
        "# Print the unique tags\n",
        "for tag in unique_tags:\n",
        "    print(tag)\n",
        "\n",
        "# Delete the variable doc to save space as we have alreay extracted the necessary data we need.\n",
        "del doc"
      ],
      "metadata": {
        "id": "AcrOfaQfhDVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324b7a6b-139a-4f22-e895-7aa910ad73f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title\n",
            "speaker\n",
            "content-type\n",
            "transcriber\n",
            "reviewer\n",
            "url\n",
            "dtime\n",
            "keywords\n",
            "talkid\n",
            "xml\n",
            "encoding\n",
            "content\n",
            "videopath\n",
            "seekvideo\n",
            "translators\n",
            "pagesize\n",
            "head\n",
            "file\n",
            "transcription\n",
            "videourl\n",
            "reviewers\n",
            "translator\n",
            "description\n",
            "charnum\n",
            "date\n",
            "wordnum\n",
            "transcribers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9UVfpExaBtu"
      },
      "source": [
        "### Part 1: Preprocessing\n",
        "\n",
        "Although language oftentimes follows strict rules and structures, text datasets in practice are often noisy. Before we can use our text data, we therefore need to clean it. This process is called *preprocessing*. If we don't do this, the data may be inconsistent and therefore more difficult to analyze and work with. You may ask yourself, what exactly do we need to clean, where exactly would we expect inconsistencies? Take a moment and think about it potential hickups in the raw data. What we need to do, is to preprocess the text and bring it into a clean and consistent format, allowing further analyzing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKWHxHG7bkWa"
      },
      "source": [
        "<h4>Exercise 1.1 (3 Points)</h4>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following part of the code shows a chunk of text from our ted text dataset. Have a look and try to identify **three** issues you can think of that can create a problem for text analysis, next to the one which is already provided. When giving your answer, we ask you to also mention **why** the issues could be problematic (similar to the example given). Each correct answer (naming + explaining the problem) will give 1 point."
      ],
      "metadata": {
        "id": "Lf3LEeuDVcs4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpx6RFzaBtv",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29c0855-8838-4f10-ec1e-65c2306e3d11"
      },
      "source": [
        "# Have a look at the output of this code, to see some text examples.\n",
        "i = input_text.find(\"Hyowon Gweon: See this?\")\n",
        "print(input_text[i:i+300])\n",
        "print()\n",
        "\n",
        "i = input_text.find(\"You will earn\")\n",
        "print(input_text[i:i+45])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyowon Gweon: See this? (Ball squeaks) Did you see that? (Ball squeaks) Cool. See this one? (Ball squeaks) Wow.\n",
            "Laura Schulz: Told you. (Laughs)\n",
            "(Video) HG: See this one? (Ball squeaks) Hey Clara, this one's for you. You can go ahead and play. (Laughter)\n",
            "LS: I don't even have to talk, right? All rig\n",
            "\n",
            "You will earn 10% of any gold you distributes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33t3GOGqcvmZ"
      },
      "source": [
        "<b>Your Solution goes here:</b>\n",
        "<br>- Speakers' names: embeddings for names will dominate the embedding space unnecessarily.\n",
        "<br>- (Output sentences are incomplete (\"All rig\") -> because of wrong array)\n",
        "<br>- Background noises are inlcuded \"(Ball squeaks)\"\n",
        "<br>- punctuation marks: are unnecessary work when searching a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Exercise 1.2 (9 Points)</h4>\n",
        "\n",
        "Next we want to create a preprocessing pipeline to later clean the entire dataset in one go. The pipeline takes `input_text` as input and should provide a cleaned and ready-to-use text data called `cleaned_text`.\n",
        "\n",
        "Your task is to implement this pipeline with **three** functions that each take care of **one** of the **three** issues you listed in exercise 1.1.\n",
        "\n",
        "Some hints about the pipeline are given as well as the code for the example from 1.1."
      ],
      "metadata": {
        "id": "3F5MFEDWt-WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import re\n",
        "import string\n",
        "\n",
        "def remove_speaker(text):\n",
        "  ''' takes the text as an input and removes the name of the speaker as output '''\n",
        "  #input_text = text\n",
        "\n",
        "  X = []\n",
        "  for line in input_text.split('\\n'):\n",
        "      #print(line)\n",
        "      m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
        "      X.extend(m.groupdict()['postcolon'])\n",
        "  without_speaker=\"\".join(X)\n",
        "  return without_speaker\n",
        "\n",
        "\n",
        "# The three functions:\n",
        "\n",
        "def remove_noises(input_text): # takes text as input and removes background noises (like \"(laughs)\")\n",
        "  text = re.sub(r'\\(\\)', '', input_text) # remove anything with parentheses in the dataset\n",
        "  return text\n",
        "\n",
        "def remove_punctuations(input_text): # removes punctuation marks\n",
        "  text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text\n",
        "\n",
        "# def ...\n",
        "\n",
        "\n",
        "\n",
        "def text_cleaned(input_text):\n",
        "\n",
        "  ''' takes the raw text as input. Runs the text through cleaning functions.\n",
        "       outputs a clean an preprocessed text for further analysis. '''\n",
        "\n",
        "  # Our three functions:\n",
        "  text_no_speaker = remove_speaker(input_text)\n",
        "  text_no_noises = remove_noises(text_no_speaker)\n",
        "  text_no_punctuations = remove_punctuations(text_no_noises)\n",
        "  text_cleaned = text_no_punctuations\n",
        "\n",
        "  return text_cleaned\n",
        "\n",
        "input_text_cleaned = text_cleaned(input_text)\n",
        "print(input_text_cleaned[i:i+200])"
      ],
      "metadata": {
        "id": "Ow4UqreOvXH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114bc27e-d855-4abc-c9a5-4b384f775d22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " theyre watching their shows actually they are being watched They are being watched by Roy Price and his team who record everything They record when somebody presses play when somebody presses pause w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX5lFA4XijPm"
      },
      "source": [
        "<h4>Exercise 1.3 (6 Points)</h4>\n",
        "\n",
        "To continue with building our embedding, we need to tokenize every single word (so that the model has individual tokens to process). Therefore we first need to split the text into sentences and after that into words.\n",
        "Try it yourselves or use the NLTK-Tools build for this (https://www.kite.com/python/docs/nltk.word_tokenize + https://www.kite.com/python/docs/nltk.sent_tokenize).\n",
        "To make it easier, we should also delete every character that is not a letter. Additionally, we could decrease the size of our vocabulary. A way to do this is by converting capital characters to lower case characters (but it also has some drawbacks - more on this in exercise 1.4).<br>\n",
        "\n",
        "Split your text into sentences and save them in the array `sentences_strings_ted`.\n",
        "Save one variabale `tokens` with all the tokens in the text and one array named `sentences_ted` that contains an array for every sentence, with all the tokenized words of that sentence.<br><br>\n",
        "Example:<br>\n",
        "If the text looks like this: \"I love cake. You have to be honest, you love it too!\", the variables should look like:<br>\n",
        "sentences_strings_ted=['I love cake.', 'You have to be honest, you love it too!']<br>\n",
        "sentences_ted=[['i', 'love', 'cake'], ['you', 'have', 'to', 'be', 'honest', 'you', 'love', 'it', 'too']]<br>\n",
        "tokens=['i', 'love', 'cake', 'you', 'have', 'to', 'be', 'honest', 'you', 'love', 'it', 'too']<br><br>\n",
        "\n",
        "IMPORTANT: Apply this to `input_text_clean`.<br><br>\n",
        "\n",
        "\n",
        " **[Hint:]** use pickle file (.pkl) to dump and load the variables like `sentences_strings_ted`, `tokens`, `sentences_ted` to continue where you left, when you comeback next time. It will save a lot of time/effort."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zAfwo7Dcvml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f95180c-d12c-44cb-b04f-86213963daae"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# sentences_strings_ted:\n",
        "sentences_strings_ted = nltk.sent_tokenize(input_text_cleaned)\n",
        "\n",
        "# tokens:\n",
        "tokens = nltk.word_tokenize(input_text_cleaned)\n",
        "\n",
        "# sentences_ted:\n",
        "def create_sentences_array(input_text_cleaned):\n",
        "    sentences_ted = nltk.sent_tokenize(input_text_cleaned)\n",
        "    return sentences_ted\n",
        "\n",
        "sentences_array = create_sentences_array(input_text_cleaned)\n",
        "\n",
        "# print(nltk.word_tokenize(input_text_cleaned))\n",
        "\n",
        "\n",
        "'''\n",
        "# print the sentences and tokens:\n",
        "print(\"sentences_strings_ted:\")\n",
        "print(sentences_strings_ted)\n",
        "\n",
        "print(\"\\n sentences_ted:\")\n",
        "for sentence in sentences_array[0:10]:\n",
        "  print(sentence)\n",
        "\n",
        "print(\"\\n tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# limit the output\n",
        "PAGE_SIZE = 1\n",
        "for i in range(0, len(sentences_array), PAGE_SIZE):\n",
        "    print(sentences_array[i:i+PAGE_SIZE])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pickle #pickle file to dump & load the variable\n",
        "\n",
        "pickle_file_path = 'tokenization.pkl'\n",
        "\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "  pickle.dump((sentences_strings_ted, sentences_array), pickle_file)\n",
        "\n",
        "with open('preprocessed_data.pkl', 'rb') as pickle_file:\n",
        "  sentences_strings_ted, sentences_array = pickle.load(pickle_file)'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the output to a file\n",
        "output_filename = \"output.txt\"\n",
        "with open(output_filename, \"w\") as f:\n",
        "    f.write(\"\\n\".join(sentences_array))\n",
        "\n",
        "# Display a link to download the file\n",
        "FileLink(output_filename)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/output.txt"
            ],
            "text/html": [
              "<a href='output.txt' target='_blank'>output.txt</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUiFC0I5j31i"
      },
      "source": [
        "<h4>Exercise 1.4 (1 Point)</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The good side of converting all capital letters is, that we reduce the volume of the vocabulary. Thereby we dont differentiate between the the words \"today\" and \"Today\". But there is a caveat. Can you think of any downside to this process?"
      ],
      "metadata": {
        "id": "7OICTv5Ybj9Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk6zmQjNcvmo"
      },
      "source": [
        "**Your answer goes here:**\n",
        "By converting all capital letters, we lose information about the meaning and the part of speech of the word, so for instance if it is a noun or an adjective etc.\n",
        "Also, it can cause contextual ambiguity."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly see how large our vocabulary turned out to be!"
      ],
      "metadata": {
        "id": "e_8Pg_RQYglD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooe1gfSZaBuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "29ddda5f-6328-445f-a3b8-30401b112d8a"
      },
      "source": [
        "with open(output_filename, \"r\") as f:\n",
        "    loaded_data = f.read()\n",
        "\n",
        "len(sentences_ted)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e9e40c8cae21>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloaded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_ted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences_ted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjkAGHjaBuN"
      },
      "source": [
        "### Part 2: Word Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLMH0iXaBuO"
      },
      "source": [
        "<h4>Exercise 2.1 (2 Points)</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your next task will be to store the counts of the top 1,000 most frequent words in a list called `counts_ted_top1000` ! There are multiple ways to do this. You can have a look at the Counter-Function (https://docs.python.org/2/library/collections.html) or the FreqDist-Function (https://www.kite.com/python/docs/nltk.FreqDist). If you don't trust any of these, you can of course build your own function. In the end we want an array with tuples of the structure:\n",
        "\n",
        "counts_ted_top1000 =  [(WordA,FrequencyA),(WordB,FrequencyB)]"
      ],
      "metadata": {
        "id": "Pm9092DdYskT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcQzY8iWaBuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e170e5dd-b223-4f25-8988-0f7b8b498a59"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "freq_dist = FreqDist(tokens)\n",
        "counts_ted_top1000 = freq_dist.most_common(1000)\n",
        "\n",
        "# Print the top 1000 most frequent words\n",
        "print(\"The 1000 most frequent words are:\")\n",
        "for word, frequency in counts_ted_top1000:\n",
        "    print(f\"{word}, {frequency}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 1000 most frequent words are:\n",
            "the, 195496\n",
            "to, 124137\n",
            "of, 114250\n",
            "and, 106265\n",
            "a, 102782\n",
            "that, 82038\n",
            "in, 72990\n",
            "I, 65610\n",
            "is, 62480\n",
            "you, 54134\n",
            "we, 45729\n",
            "it, 43971\n",
            "this, 40619\n",
            "And, 38248\n",
            "was, 30688\n",
            "for, 28253\n",
            "are, 27516\n",
            "have, 27092\n",
            "on, 25038\n",
            "with, 24282\n",
            "they, 21231\n",
            "about, 21020\n",
            "can, 20904\n",
            "be, 19999\n",
            "what, 19865\n",
            "not, 18644\n",
            "at, 18458\n",
            "as, 18195\n",
            "all, 17556\n",
            "do, 16970\n",
            "its, 16443\n",
            "my, 16236\n",
            "like, 15624\n",
            "people, 15440\n",
            "So, 15436\n",
            "were, 15294\n",
            "one, 15140\n",
            "from, 15136\n",
            "so, 15076\n",
            "but, 14203\n",
            "an, 13708\n",
            "our, 13411\n",
            "just, 13230\n",
            "or, 13053\n",
            "there, 12277\n",
            "these, 12274\n",
            "if, 12157\n",
            "very, 12155\n",
            "me, 11952\n",
            "out, 11876\n",
            "know, 11385\n",
            "by, 11368\n",
            "going, 11307\n",
            "them, 11214\n",
            "up, 10951\n",
            "when, 10907\n",
            "had, 10866\n",
            "because, 10757\n",
            "more, 10626\n",
            "The, 10424\n",
            "But, 10269\n",
            "think, 10166\n",
            "see, 9912\n",
            "their, 9853\n",
            "your, 9781\n",
            "We, 9726\n",
            "would, 9698\n",
            "which, 9674\n",
            "really, 9553\n",
            "how, 9528\n",
            "who, 9312\n",
            "get, 9261\n",
            "he, 8976\n",
            "us, 8490\n",
            "Im, 8423\n",
            "then, 8324\n",
            "has, 8218\n",
            "time, 8176\n",
            "Its, 8147\n",
            "some, 7889\n",
            "into, 7845\n",
            "actually, 7761\n",
            "dont, 7753\n",
            "will, 7468\n",
            "world, 7447\n",
            "way, 7443\n",
            "here, 7398\n",
            "years, 7384\n",
            "thats, 7321\n",
            "things, 7315\n",
            "where, 7255\n",
            "It, 7227\n",
            "want, 7082\n",
            "now, 7076\n",
            "other, 7046\n",
            "could, 7007\n",
            "This, 7003\n",
            "been, 6921\n",
            "go, 6856\n",
            "You, 6737\n",
            "make, 6686\n",
            "said, 6314\n",
            "something, 6103\n",
            "than, 5968\n",
            "those, 5882\n",
            "no, 5749\n",
            "right, 5679\n",
            "first, 5667\n",
            "also, 5548\n",
            "two, 5538\n",
            "got, 5498\n",
            "thing, 5426\n",
            "little, 5418\n",
            "much, 5407\n",
            "They, 5392\n",
            "look, 5231\n",
            "say, 5212\n",
            "back, 5171\n",
            "over, 5047\n",
            "only, 5004\n",
            "work, 4947\n",
            "his, 4937\n",
            "most, 4905\n",
            "need, 4894\n",
            "even, 4789\n",
            "take, 4636\n",
            "many, 4630\n",
            "life, 4533\n",
            "kind, 4526\n",
            "lot, 4522\n",
            "youre, 4478\n",
            "did, 4400\n",
            "around, 4362\n",
            "new, 4335\n",
            "she, 4263\n",
            "different, 4238\n",
            "Now, 4179\n",
            "Laughter, 4168\n",
            "good, 4151\n",
            "theres, 4110\n",
            "In, 4091\n",
            "through, 3983\n",
            "down, 3949\n",
            "every, 3938\n",
            "same, 3923\n",
            "her, 3918\n",
            "come, 3777\n",
            "being, 3749\n",
            "theyre, 3727\n",
            "What, 3723\n",
            "use, 3700\n",
            "doing, 3640\n",
            "put, 3534\n",
            "well, 3509\n",
            "Well, 3461\n",
            "If, 3412\n",
            "day, 3393\n",
            "called, 3370\n",
            "any, 3342\n",
            "percent, 3255\n",
            "three, 3225\n",
            "made, 3224\n",
            "Ive, 3120\n",
            "tell, 3092\n",
            "why, 3091\n",
            "find, 3045\n",
            "fact, 3006\n",
            "didnt, 2972\n",
            "human, 2947\n",
            "He, 2924\n",
            "started, 2894\n",
            "Thats, 2886\n",
            "talk, 2863\n",
            "change, 2863\n",
            "idea, 2847\n",
            "great, 2826\n",
            "own, 2823\n",
            "year, 2816\n",
            "after, 2792\n",
            "went, 2711\n",
            "thought, 2705\n",
            "last, 2696\n",
            "should, 2687\n",
            "might, 2686\n",
            "today, 2666\n",
            "better, 2661\n",
            "big, 2654\n",
            "give, 2653\n",
            "never, 2648\n",
            "weve, 2643\n",
            "before, 2632\n",
            "important, 2609\n",
            "able, 2608\n",
            "cant, 2569\n",
            "another, 2556\n",
            "still, 2536\n",
            "course, 2532\n",
            "part, 2472\n",
            "problem, 2463\n",
            "together, 2443\n",
            "came, 2441\n",
            "start, 2433\n",
            "him, 2431\n",
            "next, 2421\n",
            "show, 2403\n",
            "off, 2398\n",
            "system, 2394\n",
            "ago, 2387\n",
            "few, 2359\n",
            "There, 2333\n",
            "does, 2321\n",
            "story, 2318\n",
            "used, 2313\n",
            "bit, 2313\n",
            "each, 2307\n",
            "between, 2288\n",
            "again, 2274\n",
            "brain, 2273\n",
            "place, 2238\n",
            "mean, 2231\n",
            "found, 2173\n",
            "too, 2149\n",
            "question, 2136\n",
            "technology, 2136\n",
            "data, 2128\n",
            "looking, 2126\n",
            "example, 2125\n",
            "doesnt, 2112\n",
            "water, 2103\n",
            "That, 2086\n",
            "love, 2068\n",
            "wanted, 2068\n",
            "long, 2062\n",
            "done, 2053\n",
            "end, 2029\n",
            "point, 2027\n",
            "sort, 2015\n",
            "understand, 2007\n",
            "Because, 1999\n",
            "call, 1987\n",
            "ever, 1987\n",
            "always, 1977\n",
            "trying, 1958\n",
            "whole, 1958\n",
            "women, 1957\n",
            "live, 1953\n",
            "real, 1941\n",
            "feel, 1915\n",
            "When, 1912\n",
            "A, 1899\n",
            "believe, 1891\n",
            "try, 1879\n",
            "working, 1875\n",
            "away, 1869\n",
            "million, 1861\n",
            "may, 1860\n",
            "help, 1844\n",
            "How, 1819\n",
            "school, 1803\n",
            "thinking, 1797\n",
            "person, 1795\n",
            "children, 1794\n",
            "using, 1786\n",
            "four, 1768\n",
            "means, 1759\n",
            "took, 1758\n",
            "10, 1755\n",
            "information, 1738\n",
            "whats, 1732\n",
            "country, 1720\n",
            "become, 1711\n",
            "maybe, 1711\n",
            "money, 1709\n",
            "create, 1700\n",
            "kids, 1695\n",
            "power, 1673\n",
            "everything, 1669\n",
            "small, 1639\n",
            "getting, 1629\n",
            "number, 1608\n",
            "old, 1597\n",
            "quite, 1589\n",
            "enough, 1580\n",
            "home, 1578\n",
            "comes, 1578\n",
            "best, 1574\n",
            "design, 1572\n",
            "five, 1572\n",
            "times, 1565\n",
            "space, 1563\n",
            "talking, 1558\n",
            "sense, 1554\n",
            "happened, 1551\n",
            "making, 1545\n",
            "without, 1538\n",
            "second, 1537\n",
            "probably, 1525\n",
            "Id, 1523\n",
            "left, 1514\n",
            "future, 1513\n",
            "less, 1510\n",
            "Theres, 1509\n",
            "told, 1508\n",
            "am, 1498\n",
            "social, 1486\n",
            "energy, 1477\n",
            "building, 1472\n",
            "interesting, 1471\n",
            "let, 1469\n",
            "ask, 1464\n",
            "food, 1458\n",
            "light, 1442\n",
            "coming, 1440\n",
            "pretty, 1435\n",
            "countries, 1427\n",
            "These, 1424\n",
            "such, 1422\n",
            "body, 1419\n",
            "anything, 1406\n",
            "Ill, 1403\n",
            "hard, 1401\n",
            "dollars, 1401\n",
            "One, 1397\n",
            "across, 1390\n",
            "saw, 1390\n",
            "stuff, 1388\n",
            "family, 1379\n",
            "lives, 1379\n",
            "play, 1379\n",
            "Thank, 1377\n",
            "science, 1364\n",
            "makes, 1362\n",
            "Were, 1361\n",
            "asked, 1361\n",
            "build, 1354\n",
            "My, 1350\n",
            "says, 1350\n",
            "moment, 1343\n",
            "youve, 1342\n",
            "man, 1337\n",
            "having, 1337\n",
            "seen, 1331\n",
            "♫♫, 1327\n",
            "experience, 1322\n",
            "No, 1321\n",
            "living, 1317\n",
            "room, 1316\n",
            "lets, 1315\n",
            "ways, 1310\n",
            "happen, 1310\n",
            "side, 1304\n",
            "simple, 1302\n",
            "later, 1300\n",
            "days, 1298\n",
            "case, 1295\n",
            "half, 1293\n",
            "goes, 1283\n",
            "young, 1274\n",
            "reason, 1274\n",
            "happens, 1273\n",
            "men, 1273\n",
            "care, 1272\n",
            "20, 1269\n",
            "move, 1267\n",
            "Why, 1265\n",
            "almost, 1264\n",
            "learn, 1260\n",
            "while, 1246\n",
            "bad, 1245\n",
            "process, 1240\n",
            "inside, 1226\n",
            "saying, 1226\n",
            "high, 1220\n",
            "picture, 1213\n",
            "single, 1208\n",
            "looked, 1207\n",
            "city, 1206\n",
            "problems, 1205\n",
            "else, 1201\n",
            "New, 1200\n",
            "already, 1196\n",
            "Africa, 1196\n",
            "far, 1191\n",
            "computer, 1186\n",
            "health, 1184\n",
            "nothing, 1179\n",
            "myself, 1175\n",
            "both, 1174\n",
            "She, 1167\n",
            "billion, 1166\n",
            "often, 1162\n",
            "whether, 1162\n",
            "community, 1160\n",
            "set, 1160\n",
            "Theyre, 1159\n",
            "For, 1156\n",
            "once, 1153\n",
            "yet, 1151\n",
            "someone, 1146\n",
            "within, 1143\n",
            "answer, 1141\n",
            "possible, 1139\n",
            "remember, 1137\n",
            "months, 1132\n",
            "hand, 1128\n",
            "♫, 1126\n",
            "wrong, 1124\n",
            "looks, 1121\n",
            "book, 1121\n",
            "basically, 1120\n",
            "keep, 1118\n",
            "car, 1116\n",
            "project, 1113\n",
            "sure, 1112\n",
            "hope, 1111\n",
            "matter, 1107\n",
            "business, 1104\n",
            "cells, 1103\n",
            "heard, 1102\n",
            "public, 1102\n",
            "United, 1100\n",
            "planet, 1096\n",
            "bring, 1096\n",
            "global, 1096\n",
            "ideas, 1091\n",
            "mind, 1089\n",
            "hes, 1089\n",
            "imagine, 1088\n",
            "true, 1086\n",
            "wasnt, 1084\n",
            "top, 1083\n",
            "amazing, 1079\n",
            "guy, 1078\n",
            "As, 1077\n",
            "history, 1072\n",
            "six, 1064\n",
            "read, 1064\n",
            "ones, 1062\n",
            "job, 1061\n",
            "words, 1056\n",
            "open, 1053\n",
            "All, 1047\n",
            "knew, 1043\n",
            "control, 1040\n",
            "couple, 1039\n",
            "until, 1035\n",
            "friends, 1035\n",
            "group, 1031\n",
            "government, 1029\n",
            "Earth, 1023\n",
            "face, 1022\n",
            "order, 1021\n",
            "piece, 1016\n",
            "under, 1013\n",
            "built, 1012\n",
            "form, 1012\n",
            "beautiful, 1011\n",
            "child, 1007\n",
            "turn, 1005\n",
            "decided, 1005\n",
            "learned, 1001\n",
            "became, 998\n",
            "though, 997\n",
            "isnt, 992\n",
            "age, 992\n",
            "research, 987\n",
            "places, 984\n",
            "completely, 982\n",
            "taking, 974\n",
            "music, 974\n",
            "study, 972\n",
            "States, 972\n",
            "Applause, 969\n",
            "against, 967\n",
            "run, 964\n",
            "gets, 964\n",
            "Internet, 964\n",
            "exactly, 963\n",
            "line, 959\n",
            "share, 955\n",
            "since, 953\n",
            "night, 949\n",
            "stories, 949\n",
            "works, 948\n",
            "video, 948\n",
            "hear, 944\n",
            "species, 942\n",
            "word, 941\n",
            "woman, 941\n",
            "kinds, 941\n",
            "language, 940\n",
            "must, 937\n",
            "stop, 935\n",
            "questions, 934\n",
            "sometimes, 931\n",
            "model, 931\n",
            "education, 922\n",
            "30, 922\n",
            "happening, 921\n",
            "head, 915\n",
            "somebody, 915\n",
            "ourselves, 913\n",
            "themselves, 913\n",
            "created, 912\n",
            "hours, 911\n",
            "cancer, 907\n",
            "name, 907\n",
            "couldnt, 906\n",
            "huge, 905\n",
            "students, 905\n",
            "’, 903\n",
            "front, 902\n",
            "animals, 900\n",
            "company, 898\n",
            "turns, 894\n",
            "worked, 893\n",
            "itself, 891\n",
            "America, 889\n",
            "large, 887\n",
            "heart, 885\n",
            "rather, 883\n",
            "guys, 880\n",
            "everybody, 878\n",
            "minutes, 877\n",
            "society, 876\n",
            "disease, 872\n",
            "nature, 864\n",
            "least, 864\n",
            "others, 859\n",
            "instead, 856\n",
            "particular, 856\n",
            "level, 856\n",
            "100, 856\n",
            "environment, 854\n",
            "thousands, 854\n",
            "entire, 853\n",
            "per, 853\n",
            "along, 852\n",
            "Oh, 852\n",
            "figure, 849\n",
            "gave, 847\n",
            "past, 845\n",
            "universe, 843\n",
            "sound, 843\n",
            "—, 841\n",
            "lots, 836\n",
            "early, 834\n",
            "50, 830\n",
            "heres, 827\n",
            "youll, 824\n",
            "taken, 823\n",
            "everyone, 818\n",
            "outside, 815\n",
            "state, 812\n",
            "mother, 810\n",
            "systems, 807\n",
            "game, 806\n",
            "God, 806\n",
            "India, 805\n",
            "American, 804\n",
            "art, 803\n",
            "happy, 801\n",
            "learning, 798\n",
            "leave, 797\n",
            "changed, 796\n",
            "difference, 793\n",
            "war, 791\n",
            "TED, 789\n",
            "natural, 788\n",
            "news, 787\n",
            "given, 786\n",
            "takes, 785\n",
            "difficult, 784\n",
            "US, 780\n",
            "black, 778\n",
            "turned, 776\n",
            "cell, 776\n",
            "seeing, 773\n",
            "during, 766\n",
            "house, 765\n",
            "machine, 763\n",
            "behind, 763\n",
            "finally, 762\n",
            "close, 758\n",
            "third, 756\n",
            "perhaps, 755\n",
            "companies, 754\n",
            "cities, 754\n",
            "yourself, 752\n",
            "15, 751\n",
            "began, 750\n",
            "OK, 749\n",
            "easy, 748\n",
            "York, 748\n",
            "terms, 748\n",
            "realized, 746\n",
            "eyes, 745\n",
            "area, 743\n",
            "reality, 742\n",
            "simply, 740\n",
            "Lets, 738\n",
            "China, 737\n",
            "moving, 736\n",
            "beginning, 735\n",
            "needed, 735\n",
            "team, 732\n",
            "needs, 732\n",
            "parents, 728\n",
            "felt, 728\n",
            "population, 728\n",
            "century, 727\n",
            "middle, 725\n",
            "parts, 724\n",
            "Or, 723\n",
            "culture, 722\n",
            "local, 721\n",
            "air, 721\n",
            "Weve, 720\n",
            "image, 717\n",
            "Then, 716\n",
            "Okay, 714\n",
            "certain, 713\n",
            "spend, 711\n",
            "walk, 707\n",
            "seven, 706\n",
            "free, 706\n",
            "hands, 705\n",
            "view, 705\n",
            "powerful, 705\n",
            "tried, 700\n",
            "economic, 699\n",
            "grow, 697\n",
            "longer, 696\n",
            "patients, 696\n",
            "wonderful, 694\n",
            "amount, 692\n",
            "interested, 690\n",
            "spent, 687\n",
            "full, 685\n",
            "phone, 685\n",
            "political, 684\n",
            "weeks, 683\n",
            "market, 683\n",
            "deal, 680\n",
            "size, 679\n",
            "Do, 679\n",
            "common, 674\n",
            "whatever, 670\n",
            "known, 670\n",
            "week, 670\n",
            "humans, 669\n",
            "ability, 663\n",
            "media, 663\n",
            "paper, 663\n",
            "sitting, 661\n",
            "gone, 660\n",
            "quickly, 660\n",
            "lost, 660\n",
            "fish, 659\n",
            "land, 659\n",
            "death, 656\n",
            "Here, 655\n",
            "Some, 654\n",
            "cost, 653\n",
            "Youre, 653\n",
            "opportunity, 652\n",
            "changes, 651\n",
            "shows, 651\n",
            "scale, 649\n",
            "rest, 648\n",
            "worlds, 647\n",
            "oil, 645\n",
            "father, 644\n",
            "People, 642\n",
            "buy, 640\n",
            "challenge, 639\n",
            "poor, 639\n",
            "Yeah, 637\n",
            "write, 637\n",
            "Is, 634\n",
            "growth, 632\n",
            "wouldnt, 630\n",
            "eight, 630\n",
            "based, 626\n",
            "physical, 624\n",
            "feeling, 623\n",
            "DNA, 622\n",
            "field, 621\n",
            "average, 620\n",
            "complex, 619\n",
            "met, 619\n",
            "Yes, 618\n",
            "friend, 617\n",
            "structure, 616\n",
            "test, 616\n",
            "either, 615\n",
            "born, 613\n",
            "step, 613\n",
            "program, 612\n",
            "pay, 612\n",
            "areas, 610\n",
            "surface, 610\n",
            "access, 608\n",
            "brought, 608\n",
            "growing, 606\n",
            "incredible, 604\n",
            "realize, 603\n",
            "hundreds, 603\n",
            "value, 603\n",
            "morning, 602\n",
            "wrote, 602\n",
            "feet, 601\n",
            "ocean, 601\n",
            "climate, 600\n",
            "behavior, 600\n",
            "white, 599\n",
            "At, 599\n",
            "numbers, 598\n",
            "speak, 596\n",
            "blue, 595\n",
            "scientists, 594\n",
            "economy, 594\n",
            "images, 594\n",
            "seems, 589\n",
            "animal, 587\n",
            "telling, 587\n",
            "Just, 585\n",
            "literally, 584\n",
            "wont, 582\n",
            "theyve, 582\n",
            "attention, 581\n",
            "starting, 580\n",
            "die, 578\n",
            "understanding, 578\n",
            "developed, 578\n",
            "giving, 577\n",
            "girl, 572\n",
            "miles, 571\n",
            "red, 570\n",
            "green, 569\n",
            "Hes, 568\n",
            "books, 568\n",
            "support, 567\n",
            "individual, 564\n",
            "eat, 564\n",
            "tools, 563\n",
            "result, 562\n",
            "risk, 561\n",
            "running, 561\n",
            "40, 560\n",
            "Not, 560\n",
            "knowledge, 559\n",
            "millions, 559\n",
            "alone, 556\n",
            "absolutely, 554\n",
            "watch, 553\n",
            "personal, 553\n",
            "Our, 549\n",
            "technologies, 547\n",
            "key, 547\n",
            "nice, 547\n",
            "online, 546\n",
            "movement, 546\n",
            "issue, 546\n",
            "whos, 546\n",
            "hold, 544\n",
            "bottom, 543\n",
            "blood, 542\n",
            "anyone, 541\n",
            "talked, 541\n",
            "material, 540\n",
            "gives, 540\n",
            "kid, 539\n",
            "short, 536\n",
            "ground, 536\n",
            "map, 536\n",
            "deep, 534\n",
            "To, 533\n",
            "theory, 533\n",
            "discovered, 532\n",
            "seem, 531\n",
            "center, 531\n",
            "showed, 531\n",
            "playing, 531\n",
            "audience, 530\n",
            "creating, 529\n",
            "Heres, 529\n",
            "clear, 529\n",
            "fear, 528\n",
            "cut, 528\n",
            "girls, 528\n",
            "stage, 527\n",
            "shes, 527\n",
            "cars, 526\n",
            "relationship, 526\n",
            "Right, 524\n",
            "asking, 524\n",
            "World, 520\n",
            "especially, 519\n",
            "tiny, 519\n",
            "chance, 519\n",
            "recently, 518\n",
            "force, 518\n",
            "normal, 517\n",
            "medical, 517\n",
            "Europe, 517\n",
            "allow, 516\n",
            "forward, 516\n",
            "focus, 516\n",
            "rate, 515\n",
            "issues, 514\n",
            "thank, 513\n",
            "voice, 512\n",
            "English, 512\n",
            "reasons, 511\n",
            "computers, 511\n",
            "solve, 511\n",
            "developing, 511\n",
            "meet, 508\n",
            "begin, 507\n",
            "South, 506\n",
            "On, 505\n",
            "choice, 504\n",
            "knows, 504\n",
            "designed, 504\n",
            "situation, 503\n",
            "produce, 503\n",
            "likely, 501\n",
            "Whats, 501\n",
            "pictures, 501\n",
            "development, 501\n",
            "fly, 501\n",
            "becomes, 500\n",
            "industry, 500\n",
            "fun, 499\n",
            "film, 499\n",
            "solution, 498\n",
            "incredibly, 497\n",
            "Can, 497\n",
            "save, 496\n",
            "dark, 496\n",
            "digital, 496\n",
            "several, 495\n",
            "soon, 495\n",
            "color, 494\n",
            "impact, 494\n",
            "bigger, 491\n",
            "havent, 491\n",
            "available, 490\n",
            "groups, 489\n",
            "lab, 489\n",
            "changing, 488\n",
            "major, 488\n",
            "involved, 488\n",
            "beyond, 487\n",
            "experiment, 487\n",
            "peoples, 486\n",
            "network, 486\n",
            "yes, 485\n",
            "innovation, 483\n",
            "12, 483\n",
            "resources, 482\n",
            "putting, 482\n",
            "fast, 479\n",
            "writing, 479\n",
            "towards, 478\n",
            "shape, 478\n",
            "similar, 476\n",
            "basic, 476\n",
            "box, 476\n",
            "evidence, 476\n",
            "act, 475\n",
            "arent, 474\n",
            "generation, 473\n",
            "law, 473\n",
            "stand, 473\n",
            "explain, 471\n",
            "cause, 471\n",
            "robot, 470\n",
            "product, 470\n",
            "send, 470\n",
            "special, 469\n",
            "type, 469\n",
            "guess, 469\n",
            "message, 469\n",
            "sea, 469\n",
            "Chinese, 466\n",
            "hit, 465\n",
            "effect, 465\n",
            "stay, 464\n",
            "truth, 464\n",
            "lived, 464\n",
            "drug, 462\n",
            "drugs, 462\n",
            "teach, 462\n",
            "approach, 459\n",
            "dead, 459\n",
            "journey, 458\n",
            "Maybe, 458\n",
            "Google, 458\n",
            "starts, 458\n",
            "ice, 457\n",
            "died, 455\n",
            "street, 454\n",
            "perfect, 453\n",
            "drive, 453\n",
            "baby, 453\n",
            "cool, 452\n",
            "modern, 451\n",
            "communities, 451\n",
            "reach, 450\n",
            "potential, 450\n",
            "present, 449\n",
            "walking, 448\n",
            "camera, 448\n",
            "sounds, 448\n",
            "pick, 448\n",
            "hundred, 447\n",
            "particularly, 446\n",
            "device, 446\n",
            "rules, 446\n",
            "measure, 445\n",
            "patient, 445\n",
            "scientific, 444\n",
            "certainly, 444\n",
            "rights, 443\n",
            "worth, 443\n",
            "evolution, 442\n",
            "totally, 442\n",
            "software, 442\n",
            "violence, 440\n",
            "showing, 440\n",
            "examples, 438\n",
            "onto, 438\n",
            "office, 437\n",
            "crazy, 436\n",
            "games, 436\n",
            "ready, 434\n",
            "individuals, 433\n",
            "develop, 433\n",
            "biggest, 432\n",
            "moved, 431\n",
            "anybody, 431\n",
            "wants, 430\n",
            "largest, 430\n",
            "okay, 430\n",
            "eye, 429\n",
            "By, 428\n",
            "favorite, 428\n",
            "quality, 427\n",
            "25, 427\n",
            "higher, 427\n",
            "break, 425\n",
            "extremely, 424\n",
            "sit, 424\n",
            "grew, 423\n",
            "response, 423\n",
            "plants, 422\n",
            "schools, 422\n",
            "solar, 422\n",
            "among, 421\n",
            "watching, 421\n",
            "period, 421\n",
            "class, 420\n",
            "success, 419\n",
            "Every, 418\n",
            "choose, 418\n",
            "nine, 415\n",
            "named, 414\n",
            "lead, 413\n",
            "exciting, 413\n",
            "listen, 413\n",
            "month, 413\n",
            "including, 412\n",
            "youd, 412\n",
            "national, 411\n",
            "North, 411\n",
            "memory, 411\n",
            "obviously, 411\n",
            "low, 411\n",
            "supposed, 411\n",
            "movie, 410\n",
            "allowed, 410\n",
            "results, 410\n",
            "materials, 410\n",
            "continue, 409\n",
            "plant, 408\n",
            "security, 408\n",
            "action, 408\n",
            "objects, 407\n",
            "led, 406\n",
            "worse, 406\n",
            "strong, 406\n",
            "carbon, 406\n",
            "eventually, 405\n",
            "leaders, 405\n",
            "object, 405\n",
            "code, 405\n",
            "nobody, 403\n",
            "teachers, 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hti5NFdGaBuX"
      },
      "source": [
        "The following code is going to plot a histogram with the counts of the  top-30 words as bars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5BMmYmhcvmt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "d3dca8e3-61d3-4904-f0bb-d8894989a4bd"
      },
      "source": [
        "mostfreqn=30 # Here we define how many of them we want to see in the diagramm\n",
        "frequency=[y for (x,y) in counts_ted_top1000][:mostfreqn]\n",
        "word=[x for (x,y) in counts_ted_top1000][:mostfreqn]\n",
        "indices = np.arange(len(counts_ted_top1000[:mostfreqn]))\n",
        "plt.bar(indices, frequency, color='r')\n",
        "plt.xticks(indices, word, rotation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WssgORGecvmu"
      },
      "source": [
        "Exercise: You can clearly see, that many of the most common words are redundant and not very meaningful. These types of words are called **stopwords**. What problems can stop words create in the NLP and why it is important to remove them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ3Fvsteb-Hi"
      },
      "source": [
        "**Your answer goes here:**\n",
        "Stop words can decrease processing speed since there is a large number of such words in a text. Also, they impair the extraction of essential information since they occupy the storage."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xEOOEHp1n0L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Exercise 2.2 (2 Points)</h4>"
      ],
      "metadata": {
        "id": "8r19KPrAH4iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V1zR2PICnxDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, write a function that removes the stopwords from the variable `counts_ted_top1000` and save it as `counts_ted_top1000_no_stopword`. Use the code for visualization and spot the differences.\n",
        "\n",
        "The structure in the end should look like this: counts_ted_top1000_no_stopword = [(WordA,FrequencyA),(WordB,FrequencyB)]"
      ],
      "metadata": {
        "id": "imgTrP0jcCXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(counts_ted_top1000):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    counts_ted_top1000_no_stopword = [word for word in counts_ted_top1000] #if word.lower() not in stop_words]\n",
        "    return counts_ted_top1000_no_stopword\n",
        "\n",
        "result = remove_stopwords(counts_ted_top1000)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "x7dpvwqyPuzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf243501-7e02-4d8e-95b7-a5951aeb9be5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 195496), ('to', 124137), ('of', 114250), ('and', 106265), ('a', 102782), ('that', 82038), ('in', 72990), ('I', 65610), ('is', 62480), ('you', 54134), ('we', 45729), ('it', 43971), ('this', 40619), ('And', 38248), ('was', 30688), ('for', 28253), ('are', 27516), ('have', 27092), ('on', 25038), ('with', 24282), ('they', 21231), ('about', 21020), ('can', 20904), ('be', 19999), ('what', 19865), ('not', 18644), ('at', 18458), ('as', 18195), ('all', 17556), ('do', 16970), ('its', 16443), ('my', 16236), ('like', 15624), ('people', 15440), ('So', 15436), ('were', 15294), ('one', 15140), ('from', 15136), ('so', 15076), ('but', 14203), ('an', 13708), ('our', 13411), ('just', 13230), ('or', 13053), ('there', 12277), ('these', 12274), ('if', 12157), ('very', 12155), ('me', 11952), ('out', 11876), ('know', 11385), ('by', 11368), ('going', 11307), ('them', 11214), ('up', 10951), ('when', 10907), ('had', 10866), ('because', 10757), ('more', 10626), ('The', 10424), ('But', 10269), ('think', 10166), ('see', 9912), ('their', 9853), ('your', 9781), ('We', 9726), ('would', 9698), ('which', 9674), ('really', 9553), ('how', 9528), ('who', 9312), ('get', 9261), ('he', 8976), ('us', 8490), ('Im', 8423), ('then', 8324), ('has', 8218), ('time', 8176), ('Its', 8147), ('some', 7889), ('into', 7845), ('actually', 7761), ('dont', 7753), ('will', 7468), ('world', 7447), ('way', 7443), ('here', 7398), ('years', 7384), ('thats', 7321), ('things', 7315), ('where', 7255), ('It', 7227), ('want', 7082), ('now', 7076), ('other', 7046), ('could', 7007), ('This', 7003), ('been', 6921), ('go', 6856), ('You', 6737), ('make', 6686), ('said', 6314), ('something', 6103), ('than', 5968), ('those', 5882), ('no', 5749), ('right', 5679), ('first', 5667), ('also', 5548), ('two', 5538), ('got', 5498), ('thing', 5426), ('little', 5418), ('much', 5407), ('They', 5392), ('look', 5231), ('say', 5212), ('back', 5171), ('over', 5047), ('only', 5004), ('work', 4947), ('his', 4937), ('most', 4905), ('need', 4894), ('even', 4789), ('take', 4636), ('many', 4630), ('life', 4533), ('kind', 4526), ('lot', 4522), ('youre', 4478), ('did', 4400), ('around', 4362), ('new', 4335), ('she', 4263), ('different', 4238), ('Now', 4179), ('Laughter', 4168), ('good', 4151), ('theres', 4110), ('In', 4091), ('through', 3983), ('down', 3949), ('every', 3938), ('same', 3923), ('her', 3918), ('come', 3777), ('being', 3749), ('theyre', 3727), ('What', 3723), ('use', 3700), ('doing', 3640), ('put', 3534), ('well', 3509), ('Well', 3461), ('If', 3412), ('day', 3393), ('called', 3370), ('any', 3342), ('percent', 3255), ('three', 3225), ('made', 3224), ('Ive', 3120), ('tell', 3092), ('why', 3091), ('find', 3045), ('fact', 3006), ('didnt', 2972), ('human', 2947), ('He', 2924), ('started', 2894), ('Thats', 2886), ('talk', 2863), ('change', 2863), ('idea', 2847), ('great', 2826), ('own', 2823), ('year', 2816), ('after', 2792), ('went', 2711), ('thought', 2705), ('last', 2696), ('should', 2687), ('might', 2686), ('today', 2666), ('better', 2661), ('big', 2654), ('give', 2653), ('never', 2648), ('weve', 2643), ('before', 2632), ('important', 2609), ('able', 2608), ('cant', 2569), ('another', 2556), ('still', 2536), ('course', 2532), ('part', 2472), ('problem', 2463), ('together', 2443), ('came', 2441), ('start', 2433), ('him', 2431), ('next', 2421), ('show', 2403), ('off', 2398), ('system', 2394), ('ago', 2387), ('few', 2359), ('There', 2333), ('does', 2321), ('story', 2318), ('used', 2313), ('bit', 2313), ('each', 2307), ('between', 2288), ('again', 2274), ('brain', 2273), ('place', 2238), ('mean', 2231), ('found', 2173), ('too', 2149), ('question', 2136), ('technology', 2136), ('data', 2128), ('looking', 2126), ('example', 2125), ('doesnt', 2112), ('water', 2103), ('That', 2086), ('love', 2068), ('wanted', 2068), ('long', 2062), ('done', 2053), ('end', 2029), ('point', 2027), ('sort', 2015), ('understand', 2007), ('Because', 1999), ('call', 1987), ('ever', 1987), ('always', 1977), ('trying', 1958), ('whole', 1958), ('women', 1957), ('live', 1953), ('real', 1941), ('feel', 1915), ('When', 1912), ('A', 1899), ('believe', 1891), ('try', 1879), ('working', 1875), ('away', 1869), ('million', 1861), ('may', 1860), ('help', 1844), ('How', 1819), ('school', 1803), ('thinking', 1797), ('person', 1795), ('children', 1794), ('using', 1786), ('four', 1768), ('means', 1759), ('took', 1758), ('10', 1755), ('information', 1738), ('whats', 1732), ('country', 1720), ('become', 1711), ('maybe', 1711), ('money', 1709), ('create', 1700), ('kids', 1695), ('power', 1673), ('everything', 1669), ('small', 1639), ('getting', 1629), ('number', 1608), ('old', 1597), ('quite', 1589), ('enough', 1580), ('home', 1578), ('comes', 1578), ('best', 1574), ('design', 1572), ('five', 1572), ('times', 1565), ('space', 1563), ('talking', 1558), ('sense', 1554), ('happened', 1551), ('making', 1545), ('without', 1538), ('second', 1537), ('probably', 1525), ('Id', 1523), ('left', 1514), ('future', 1513), ('less', 1510), ('Theres', 1509), ('told', 1508), ('am', 1498), ('social', 1486), ('energy', 1477), ('building', 1472), ('interesting', 1471), ('let', 1469), ('ask', 1464), ('food', 1458), ('light', 1442), ('coming', 1440), ('pretty', 1435), ('countries', 1427), ('These', 1424), ('such', 1422), ('body', 1419), ('anything', 1406), ('Ill', 1403), ('hard', 1401), ('dollars', 1401), ('One', 1397), ('across', 1390), ('saw', 1390), ('stuff', 1388), ('family', 1379), ('lives', 1379), ('play', 1379), ('Thank', 1377), ('science', 1364), ('makes', 1362), ('Were', 1361), ('asked', 1361), ('build', 1354), ('My', 1350), ('says', 1350), ('moment', 1343), ('youve', 1342), ('man', 1337), ('having', 1337), ('seen', 1331), ('♫♫', 1327), ('experience', 1322), ('No', 1321), ('living', 1317), ('room', 1316), ('lets', 1315), ('ways', 1310), ('happen', 1310), ('side', 1304), ('simple', 1302), ('later', 1300), ('days', 1298), ('case', 1295), ('half', 1293), ('goes', 1283), ('young', 1274), ('reason', 1274), ('happens', 1273), ('men', 1273), ('care', 1272), ('20', 1269), ('move', 1267), ('Why', 1265), ('almost', 1264), ('learn', 1260), ('while', 1246), ('bad', 1245), ('process', 1240), ('inside', 1226), ('saying', 1226), ('high', 1220), ('picture', 1213), ('single', 1208), ('looked', 1207), ('city', 1206), ('problems', 1205), ('else', 1201), ('New', 1200), ('already', 1196), ('Africa', 1196), ('far', 1191), ('computer', 1186), ('health', 1184), ('nothing', 1179), ('myself', 1175), ('both', 1174), ('She', 1167), ('billion', 1166), ('often', 1162), ('whether', 1162), ('community', 1160), ('set', 1160), ('Theyre', 1159), ('For', 1156), ('once', 1153), ('yet', 1151), ('someone', 1146), ('within', 1143), ('answer', 1141), ('possible', 1139), ('remember', 1137), ('months', 1132), ('hand', 1128), ('♫', 1126), ('wrong', 1124), ('looks', 1121), ('book', 1121), ('basically', 1120), ('keep', 1118), ('car', 1116), ('project', 1113), ('sure', 1112), ('hope', 1111), ('matter', 1107), ('business', 1104), ('cells', 1103), ('heard', 1102), ('public', 1102), ('United', 1100), ('planet', 1096), ('bring', 1096), ('global', 1096), ('ideas', 1091), ('mind', 1089), ('hes', 1089), ('imagine', 1088), ('true', 1086), ('wasnt', 1084), ('top', 1083), ('amazing', 1079), ('guy', 1078), ('As', 1077), ('history', 1072), ('six', 1064), ('read', 1064), ('ones', 1062), ('job', 1061), ('words', 1056), ('open', 1053), ('All', 1047), ('knew', 1043), ('control', 1040), ('couple', 1039), ('until', 1035), ('friends', 1035), ('group', 1031), ('government', 1029), ('Earth', 1023), ('face', 1022), ('order', 1021), ('piece', 1016), ('under', 1013), ('built', 1012), ('form', 1012), ('beautiful', 1011), ('child', 1007), ('turn', 1005), ('decided', 1005), ('learned', 1001), ('became', 998), ('though', 997), ('isnt', 992), ('age', 992), ('research', 987), ('places', 984), ('completely', 982), ('taking', 974), ('music', 974), ('study', 972), ('States', 972), ('Applause', 969), ('against', 967), ('run', 964), ('gets', 964), ('Internet', 964), ('exactly', 963), ('line', 959), ('share', 955), ('since', 953), ('night', 949), ('stories', 949), ('works', 948), ('video', 948), ('hear', 944), ('species', 942), ('word', 941), ('woman', 941), ('kinds', 941), ('language', 940), ('must', 937), ('stop', 935), ('questions', 934), ('sometimes', 931), ('model', 931), ('education', 922), ('30', 922), ('happening', 921), ('head', 915), ('somebody', 915), ('ourselves', 913), ('themselves', 913), ('created', 912), ('hours', 911), ('cancer', 907), ('name', 907), ('couldnt', 906), ('huge', 905), ('students', 905), ('’', 903), ('front', 902), ('animals', 900), ('company', 898), ('turns', 894), ('worked', 893), ('itself', 891), ('America', 889), ('large', 887), ('heart', 885), ('rather', 883), ('guys', 880), ('everybody', 878), ('minutes', 877), ('society', 876), ('disease', 872), ('nature', 864), ('least', 864), ('others', 859), ('instead', 856), ('particular', 856), ('level', 856), ('100', 856), ('environment', 854), ('thousands', 854), ('entire', 853), ('per', 853), ('along', 852), ('Oh', 852), ('figure', 849), ('gave', 847), ('past', 845), ('universe', 843), ('sound', 843), ('—', 841), ('lots', 836), ('early', 834), ('50', 830), ('heres', 827), ('youll', 824), ('taken', 823), ('everyone', 818), ('outside', 815), ('state', 812), ('mother', 810), ('systems', 807), ('game', 806), ('God', 806), ('India', 805), ('American', 804), ('art', 803), ('happy', 801), ('learning', 798), ('leave', 797), ('changed', 796), ('difference', 793), ('war', 791), ('TED', 789), ('natural', 788), ('news', 787), ('given', 786), ('takes', 785), ('difficult', 784), ('US', 780), ('black', 778), ('turned', 776), ('cell', 776), ('seeing', 773), ('during', 766), ('house', 765), ('machine', 763), ('behind', 763), ('finally', 762), ('close', 758), ('third', 756), ('perhaps', 755), ('companies', 754), ('cities', 754), ('yourself', 752), ('15', 751), ('began', 750), ('OK', 749), ('easy', 748), ('York', 748), ('terms', 748), ('realized', 746), ('eyes', 745), ('area', 743), ('reality', 742), ('simply', 740), ('Lets', 738), ('China', 737), ('moving', 736), ('beginning', 735), ('needed', 735), ('team', 732), ('needs', 732), ('parents', 728), ('felt', 728), ('population', 728), ('century', 727), ('middle', 725), ('parts', 724), ('Or', 723), ('culture', 722), ('local', 721), ('air', 721), ('Weve', 720), ('image', 717), ('Then', 716), ('Okay', 714), ('certain', 713), ('spend', 711), ('walk', 707), ('seven', 706), ('free', 706), ('hands', 705), ('view', 705), ('powerful', 705), ('tried', 700), ('economic', 699), ('grow', 697), ('longer', 696), ('patients', 696), ('wonderful', 694), ('amount', 692), ('interested', 690), ('spent', 687), ('full', 685), ('phone', 685), ('political', 684), ('weeks', 683), ('market', 683), ('deal', 680), ('size', 679), ('Do', 679), ('common', 674), ('whatever', 670), ('known', 670), ('week', 670), ('humans', 669), ('ability', 663), ('media', 663), ('paper', 663), ('sitting', 661), ('gone', 660), ('quickly', 660), ('lost', 660), ('fish', 659), ('land', 659), ('death', 656), ('Here', 655), ('Some', 654), ('cost', 653), ('Youre', 653), ('opportunity', 652), ('changes', 651), ('shows', 651), ('scale', 649), ('rest', 648), ('worlds', 647), ('oil', 645), ('father', 644), ('People', 642), ('buy', 640), ('challenge', 639), ('poor', 639), ('Yeah', 637), ('write', 637), ('Is', 634), ('growth', 632), ('wouldnt', 630), ('eight', 630), ('based', 626), ('physical', 624), ('feeling', 623), ('DNA', 622), ('field', 621), ('average', 620), ('complex', 619), ('met', 619), ('Yes', 618), ('friend', 617), ('structure', 616), ('test', 616), ('either', 615), ('born', 613), ('step', 613), ('program', 612), ('pay', 612), ('areas', 610), ('surface', 610), ('access', 608), ('brought', 608), ('growing', 606), ('incredible', 604), ('realize', 603), ('hundreds', 603), ('value', 603), ('morning', 602), ('wrote', 602), ('feet', 601), ('ocean', 601), ('climate', 600), ('behavior', 600), ('white', 599), ('At', 599), ('numbers', 598), ('speak', 596), ('blue', 595), ('scientists', 594), ('economy', 594), ('images', 594), ('seems', 589), ('animal', 587), ('telling', 587), ('Just', 585), ('literally', 584), ('wont', 582), ('theyve', 582), ('attention', 581), ('starting', 580), ('die', 578), ('understanding', 578), ('developed', 578), ('giving', 577), ('girl', 572), ('miles', 571), ('red', 570), ('green', 569), ('Hes', 568), ('books', 568), ('support', 567), ('individual', 564), ('eat', 564), ('tools', 563), ('result', 562), ('risk', 561), ('running', 561), ('40', 560), ('Not', 560), ('knowledge', 559), ('millions', 559), ('alone', 556), ('absolutely', 554), ('watch', 553), ('personal', 553), ('Our', 549), ('technologies', 547), ('key', 547), ('nice', 547), ('online', 546), ('movement', 546), ('issue', 546), ('whos', 546), ('hold', 544), ('bottom', 543), ('blood', 542), ('anyone', 541), ('talked', 541), ('material', 540), ('gives', 540), ('kid', 539), ('short', 536), ('ground', 536), ('map', 536), ('deep', 534), ('To', 533), ('theory', 533), ('discovered', 532), ('seem', 531), ('center', 531), ('showed', 531), ('playing', 531), ('audience', 530), ('creating', 529), ('Heres', 529), ('clear', 529), ('fear', 528), ('cut', 528), ('girls', 528), ('stage', 527), ('shes', 527), ('cars', 526), ('relationship', 526), ('Right', 524), ('asking', 524), ('World', 520), ('especially', 519), ('tiny', 519), ('chance', 519), ('recently', 518), ('force', 518), ('normal', 517), ('medical', 517), ('Europe', 517), ('allow', 516), ('forward', 516), ('focus', 516), ('rate', 515), ('issues', 514), ('thank', 513), ('voice', 512), ('English', 512), ('reasons', 511), ('computers', 511), ('solve', 511), ('developing', 511), ('meet', 508), ('begin', 507), ('South', 506), ('On', 505), ('choice', 504), ('knows', 504), ('designed', 504), ('situation', 503), ('produce', 503), ('likely', 501), ('Whats', 501), ('pictures', 501), ('development', 501), ('fly', 501), ('becomes', 500), ('industry', 500), ('fun', 499), ('film', 499), ('solution', 498), ('incredibly', 497), ('Can', 497), ('save', 496), ('dark', 496), ('digital', 496), ('several', 495), ('soon', 495), ('color', 494), ('impact', 494), ('bigger', 491), ('havent', 491), ('available', 490), ('groups', 489), ('lab', 489), ('changing', 488), ('major', 488), ('involved', 488), ('beyond', 487), ('experiment', 487), ('peoples', 486), ('network', 486), ('yes', 485), ('innovation', 483), ('12', 483), ('resources', 482), ('putting', 482), ('fast', 479), ('writing', 479), ('towards', 478), ('shape', 478), ('similar', 476), ('basic', 476), ('box', 476), ('evidence', 476), ('act', 475), ('arent', 474), ('generation', 473), ('law', 473), ('stand', 473), ('explain', 471), ('cause', 471), ('robot', 470), ('product', 470), ('send', 470), ('special', 469), ('type', 469), ('guess', 469), ('message', 469), ('sea', 469), ('Chinese', 466), ('hit', 465), ('effect', 465), ('stay', 464), ('truth', 464), ('lived', 464), ('drug', 462), ('drugs', 462), ('teach', 462), ('approach', 459), ('dead', 459), ('journey', 458), ('Maybe', 458), ('Google', 458), ('starts', 458), ('ice', 457), ('died', 455), ('street', 454), ('perfect', 453), ('drive', 453), ('baby', 453), ('cool', 452), ('modern', 451), ('communities', 451), ('reach', 450), ('potential', 450), ('present', 449), ('walking', 448), ('camera', 448), ('sounds', 448), ('pick', 448), ('hundred', 447), ('particularly', 446), ('device', 446), ('rules', 446), ('measure', 445), ('patient', 445), ('scientific', 444), ('certainly', 444), ('rights', 443), ('worth', 443), ('evolution', 442), ('totally', 442), ('software', 442), ('violence', 440), ('showing', 440), ('examples', 438), ('onto', 438), ('office', 437), ('crazy', 436), ('games', 436), ('ready', 434), ('individuals', 433), ('develop', 433), ('biggest', 432), ('moved', 431), ('anybody', 431), ('wants', 430), ('largest', 430), ('okay', 430), ('eye', 429), ('By', 428), ('favorite', 428), ('quality', 427), ('25', 427), ('higher', 427), ('break', 425), ('extremely', 424), ('sit', 424), ('grew', 423), ('response', 423), ('plants', 422), ('schools', 422), ('solar', 422), ('among', 421), ('watching', 421), ('period', 421), ('class', 420), ('success', 419), ('Every', 418), ('choose', 418), ('nine', 415), ('named', 414), ('lead', 413), ('exciting', 413), ('listen', 413), ('month', 413), ('including', 412), ('youd', 412), ('national', 411), ('North', 411), ('memory', 411), ('obviously', 411), ('low', 411), ('supposed', 411), ('movie', 410), ('allowed', 410), ('results', 410), ('materials', 410), ('continue', 409), ('plant', 408), ('security', 408), ('action', 408), ('objects', 407), ('led', 406), ('worse', 406), ('strong', 406), ('carbon', 406), ('eventually', 405), ('leaders', 405), ('object', 405), ('code', 405), ('nobody', 403), ('teachers', 403)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostfreqn=30 # Here we define how many of them we want to see in the diagramm\n",
        "frequency=[y for (x,y) in counts_ted_top1000_no_stopword][:mostfreqn]\n",
        "word=[x for (x,y) in counts_ted_top1000_no_stopword][:mostfreqn]\n",
        "indices = np.arange(len(counts_ted_top1000_no_stopword[:mostfreqn]))\n",
        "plt.bar(indices, frequency, color='r')\n",
        "plt.xticks(indices, word, rotation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tSo2313hK8n2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "35ac3a09-229d-4978-92a6-d1a4e97206d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-64a17aa31367>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmostfreqn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m \u001b[0;31m# Here we define how many of them we want to see in the diagramm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts_ted_top1000_no_stopword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmostfreqn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts_ted_top1000_no_stopword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmostfreqn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_ted_top1000_no_stopword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmostfreqn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'counts_ted_top1000_no_stopword' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wordcloud Visualization"
      ],
      "metadata": {
        "id": "ntM-fNW1cVPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below so-called **wordcloud** shows the most frequent words in a larger font and the less frequent ones in a smaller font size. It's a quick and cool way of visualizing the most frequent words!"
      ],
      "metadata": {
        "id": "zp5LcPxWvIRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps words to their frequencies\n",
        "counts_ted_top1000_no_stopword = {word: count for word, count in counts_ted_top1000_no_stopword}\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud.generate_from_frequencies(counts_ted_top1000_no_stopword)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W1AucK5tMc0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "d73df391-8613-4bde-ddbf-a4aefa590a5d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-893326c06abc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a dictionary that maps words to their frequencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcounts_ted_top1000_no_stopword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts_ted_top1000_no_stopword\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a WordCloud object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'counts_ted_top1000_no_stopword' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oRdD3e4aBud"
      },
      "source": [
        "### Part 3: Generating the Word Embeddings with Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp0JGmKWhrNd"
      },
      "source": [
        "Now it is time to run the embedding model. Gensim has an already implemented model that you can use. Using the provided model is enough for the purposes of our notebook. If you want to dive deeper into the topic - this youtube video https://www.youtube.com/watch?v=kKDYtZfriI8 could be a great guidance for you to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_t4-aiTaBue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "c2538394-8791-4095-edc2-abd2a485ba0f"
      },
      "source": [
        "# This takes a moment.. dont worry :D\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model_ted = Word2Vec(sentences_ted)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e6f123dec76d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_ted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_ted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences_ted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKYd7ZemaBuj"
      },
      "source": [
        "### Part 4: Inspection of our learned representations/embeddings (3 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7VvU82AaBuj"
      },
      "source": [
        "Now that we have a model that captures the word embeddings, we can use it to explore properties of the words in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFV06B1xcvm2"
      },
      "source": [
        "First, code a line that looks at the embedding of one individual word/token.  What does the representation of \"house\" look like in the embedding model? You may refer to the following gensim docs for functions, that might help you https://radimrehurek.com/gensim/models/keyedvectors.html). This will give you 1 point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tec0Q5WBcvm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120ed317-6880-4e46-f712-e26afc186211"
      },
      "source": [
        "word = \"house\"\n",
        "\n",
        "word_embedding = word_vectors[word]\n",
        "print(f\"The embedding for the word '{word}' is: {word_embedding}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding for the word 'house' is: [-0.18867   -0.040943   0.58878    0.11062    0.14236    0.4885\n",
            " -0.31832    0.53819   -0.018549   0.029687   0.30299   -0.16522\n",
            " -0.18896    0.5148    -0.79405    0.26409    0.027747   0.041163\n",
            " -0.49378   -0.14263    0.29017   -0.25369    0.70559   -1.0501\n",
            " -0.49344   -0.37148   -0.85796   -0.55158   -0.60251   -0.0099676\n",
            "  0.8725     0.12149    0.551      0.49924   -0.3088     1.1067\n",
            " -0.15494   -0.29923    0.91149    0.19859   -0.73946   -1.0182\n",
            "  0.37208   -0.10043    0.13537   -0.52687   -0.60437   -0.15906\n",
            "  0.49283   -0.61386    0.046815  -0.88806    0.60229    0.72199\n",
            " -0.4316    -3.0706    -0.11233   -0.45713    0.95737    0.59174\n",
            " -0.17124    0.65746    0.44741    0.6101     1.0216    -0.2458\n",
            "  0.90191    0.78319    0.28272   -0.4539     0.16309   -0.0078932\n",
            " -0.27714   -0.87249   -0.19716   -0.076285  -0.28422   -0.089584\n",
            " -1.3132     0.16372   -0.25441   -0.076529   0.44458   -0.17525\n",
            " -0.74084   -0.25415    0.52886   -0.46958    0.16487   -0.57443\n",
            "  0.47239   -0.52798    0.65184    0.803     -0.93156   -0.055967\n",
            "  0.26932    0.16221    1.1238    -0.4168   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8yGLeYpcvm6"
      },
      "source": [
        "The next task for you is to output the most similar word to \"town\"? This will also give you 1 point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs4MIsdpcvm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22755fd-1d8a-4f9f-cbc0-08750e7763e4"
      },
      "source": [
        "word = \"town\"\n",
        "\n",
        "\n",
        "similar_words = word_vectors.most_similar(word)\n",
        "most_similar_word, similarity_score = similar_words[0]\n",
        "print(f\"The most similar word to '{word}' is '{most_similar_word}'.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar word to 'town' is 'village'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCdHf0Sdcvm8"
      },
      "source": [
        "Finally, we want to find out how similar the words \"town\" and \"house\" are. Again: 1 point for this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E9opXOTcvm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c935b0d-2aa1-4f49-d14f-b4c50cb1b7c5"
      },
      "source": [
        "word1 = \"town\"\n",
        "word2 = \"house\"\n",
        "\n",
        "similarity_score = word_vectors.similarity(word1, word2)\n",
        "print(f\"The cosine similarity between '{word1}' and '{word2}' is: {similarity_score:.4f}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between 'town' and 'house' is: 0.5310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7I5G-KpqbfH"
      },
      "source": [
        "<h4>Exercise 4.1 (3 Points)</h4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have generated our embeddings, let's test some classical ideas:\n",
        "implement the following formula. Print out the 10 words, that are most similar to this formula: <br>\n",
        "$King-Man+Woman=???$\n",
        "There are two ways of computing similarity in word embeddings:\n",
        " - https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
        " - https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar_cosmul.html\n",
        "You should try out both! In this case one of them is better, but both of them are valid methods for computing similarity in the word-space."
      ],
      "metadata": {
        "id": "dgCMx111ej8l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0y_5MLlqiMx"
      },
      "source": [
        "# Your implementation goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XcdY5s-cvnA"
      },
      "source": [
        "Exercise 4.2 (2 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected outcome (Queen) should be one of the top ten most similar words. But there are also a lot of words, that you would not expect. Think about where how these words might be connected to the formula. Take your time and understand why some of the words (luther, mary, dr, president) might be in this list."
      ],
      "metadata": {
        "id": "kejO0Tmze1tq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapTRcHce7Ua"
      },
      "source": [
        "**Your answer goes here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMtpPOVtaBup"
      },
      "source": [
        "#### t-SNE visualization\n",
        "\n",
        "We will use the t-SNE algorithm, given below, for visualization. The so-called t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised and non-linear machine learning technique. It is commonly used for visualizing high dimensional data (just like our high dimensional vectors). You do not have to understand the code, its purpose is simply to give you an idea of how the data is arranged in high dimensional space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLyQ3qoAcvnB"
      },
      "source": [
        "<h4>Exercise 4.3 (2 Points)</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the t-SNE code below, first put a list of the top 50 words (as **strings**, without **stopwords**) into a variable `words_top_ted`."
      ],
      "metadata": {
        "id": "JpHG2ON9fH2j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXstzIntcvnC"
      },
      "source": [
        "# Your implementation goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSJ1AAzEcvnC"
      },
      "source": [
        "The following code gets the corresponding vectors from the model, assuming it's called `model_ted`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-lLF1lZaBus"
      },
      "source": [
        "# This assumes words_top_ted is a list of strings, the top 250 words\n",
        "words_top_vec_ted = model_ted.wv[words_top_ted]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-NxjQVMcvnE"
      },
      "source": [
        "The next few lines are for the t-SNE visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJF5ut9aBux"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "words_top_ted_tsne = tsne.fit_transform(words_top_vec_ted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2VgYLIZaBu2"
      },
      "source": [
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"word2vec T-SNE for most common words\")\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],\n",
        "                                    x2=words_top_ted_tsne[:,1],\n",
        "                                    names=words_top_ted))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "p.add_layout(labels)\n",
        "\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it. We hope you had fun and learned something in the process :-)"
      ],
      "metadata": {
        "id": "qn3osAABfZjQ"
      }
    }
  ]
}